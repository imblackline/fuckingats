{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your job descriptions\n",
    "data = \"\"\"Degree in Computer Science, Computer Engineering, Mathematics, Physics or similar\n",
    "Good knowledge of at least one of the following languages: Python, C#, Java\n",
    "6/12 months of experience as a SW Developer in a multinational and modernly organised company or in consultancy activities is welcome\n",
    "Good knowledge of SQL\n",
    "Excellent knowledge of the Italian language and good knowledge of the English language\n",
    "Interest in learning new technologies and development methodologies.\n",
    "Strong problem solving orientation, with analytical skills and an aptitude for effective communication.\n",
    "Curiosity and proactivity complete the profile\n",
    "Knowledge of Microsoft Azure Ecosystem is appreciated\n",
    "Knowledge of REST and SOAP APIs is appreciated\n",
    "Knowledge of code communication (message broker) is appreciated\n",
    "Knowledge of microservices architecture principles is appreciated\n",
    "Knowledge of Salesforce and APEX and Visualforce languages ​​is appreciated\"\"\"\n",
    "\n",
    "df = pd.DataFrame({'job_description': [data]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    # Replace newline characters with spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Remove special characters and lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text\n",
    "\n",
    "df['cleaned'] = df['job_description'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = Rake()\n",
    "\n",
    "def extract_keywords_rake(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()\n",
    "\n",
    "df['rake_keywords'] = df['cleaned'].apply(extract_keywords_rake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned'])\n",
    "tfidf_keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "df['tfidf_keywords'] = [tfidf_keywords for _ in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_spacy_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['spacy_entities'] = df['job_description'].apply(extract_spacy_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained pipeline for token classification (NER)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "def extract_transformer_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    return [(entity['word'], entity['entity']) for entity in entities]\n",
    "\n",
    "df['transformer_entities'] = df['job_description'].apply(extract_transformer_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_keywords(row):\n",
    "    combined = set(row['rake_keywords']).union(set(row['tfidf_keywords']))\n",
    "    spacy_words = [entity[0] for entity in row['spacy_entities']]\n",
    "    return combined.union(spacy_words)\n",
    "\n",
    "df['final_keywords'] = df.apply(combine_keywords, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APEX',\n",
       " 'Computer Science, Computer Engineering',\n",
       " 'English',\n",
       " 'Italian',\n",
       " 'Java\\n6/12 months',\n",
       " 'Knowledge of Salesforce',\n",
       " 'Mathematics, Physics',\n",
       " 'Microsoft Azure Ecosystem',\n",
       " 'SQL\\nExcellent',\n",
       " 'Visualforce',\n",
       " 'analytical skills',\n",
       " 'and',\n",
       " 'apex',\n",
       " 'appreciated',\n",
       " 'appreciated knowledge',\n",
       " 'aptitude',\n",
       " 'at least one',\n",
       " 'code communication message broker',\n",
       " 'computer science computer engineering mathematics physics',\n",
       " 'consultancy activities',\n",
       " 'degree',\n",
       " 'development methodologies strong problem solving orientation',\n",
       " 'effective communication curiosity',\n",
       " 'english language interest',\n",
       " 'experience',\n",
       " 'following languages python c java 612 months',\n",
       " 'good',\n",
       " 'good knowledge',\n",
       " 'in',\n",
       " 'is',\n",
       " 'italian language',\n",
       " 'knowledge',\n",
       " 'languages',\n",
       " 'learning new technologies',\n",
       " 'least one',\n",
       " 'microservices architecture principles',\n",
       " 'microsoft azure ecosystem',\n",
       " 'modernly organised company',\n",
       " 'multinational',\n",
       " 'of',\n",
       " 'or',\n",
       " 'proactivity complete',\n",
       " 'profile knowledge',\n",
       " 'rest',\n",
       " 'salesforce',\n",
       " 'similar good knowledge',\n",
       " 'soap apis',\n",
       " 'sql excellent knowledge',\n",
       " 'sw developer',\n",
       " 'the',\n",
       " 'visualforce languages',\n",
       " 'welcome good knowledge'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['final_keywords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BlackLine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def refine_keywords(keywords):\n",
    "    refined = []\n",
    "    for keyword in keywords:\n",
    "        # Remove newline remnants and split phrases into meaningful chunks\n",
    "        keyword = re.sub(r'\\s+', ' ', keyword.strip())\n",
    "        words = keyword.split()\n",
    "        meaningful_words = [word for word in words if word.lower() not in stop_words]\n",
    "        refined.append(' '.join(meaningful_words))\n",
    "    return list(set(refined))\n",
    "\n",
    "df['refined_keywords'] = df['final_keywords'].apply(refine_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_keywords = df['refined_keywords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list = [\n",
    "    # Programming Languages\n",
    "    \"javascript\", \"typescript\", \"python\", \"java\", \"c++\", \"c#\", \"ruby\", \"php\", \"swift\", \n",
    "    \"kotlin\", \"go\", \"r\", \"sql\", \"bash\", \"html\", \"css\", \"sass\", \"less\",\n",
    "\n",
    "    # Front-End Frameworks and Libraries\n",
    "    \"react\", \"vue\", \"angular\", \"svelte\", \"jquery\", \"bootstrap\", \"tailwindcss\", \"foundation\",\n",
    "    \"material-ui\", \"semantic-ui\", \"next.js\", \"nuxt.js\", \"three.js\", \"d3.js\",\n",
    "\n",
    "    # Back-End Frameworks and Libraries\n",
    "    \"node.js\", \"express\", \"django\", \"flask\", \"ruby on rails\", \"spring\", \"dotnet\", \"laravel\",\n",
    "    \"fastapi\", \"koa\", \"graphql\", \"nestjs\", \"phoenix\", \"gin\",\n",
    "\n",
    "    # State Management\n",
    "    \"redux\", \"mobx\", \"vuex\", \"recoil\", \"zustand\", \"pinia\",\n",
    "\n",
    "    # Testing Frameworks\n",
    "    \"jest\", \"mocha\", \"chai\", \"jasmine\", \"cypress\", \"puppeteer\", \"playwright\", \"karma\", \n",
    "    \"enzyme\", \"pytest\", \"unittest\", \"selenium\",\n",
    "\n",
    "    # Version Control and Collaboration\n",
    "    \"git\", \"github\", \"gitlab\", \"bitbucket\", \"svn\", \"mercurial\",\n",
    "\n",
    "    # Build Tools and Package Managers\n",
    "    \"webpack\", \"parcel\", \"rollup\", \"gulp\", \"grunt\", \"vite\", \"npm\", \"yarn\", \"pnpm\",\n",
    "\n",
    "    # Cloud and DevOps Tools\n",
    "    \"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"jenkins\", \"circleci\", \"travisci\",\n",
    "    \"ansible\", \"terraform\", \"chef\", \"puppet\", \"vagrant\",\n",
    "\n",
    "    # Databases\n",
    "    \"mysql\", \"postgresql\", \"mongodb\", \"redis\", \"sqlite\", \"cassandra\", \"firebase\",\n",
    "    \"elasticsearch\", \"dynamodb\", \"couchdb\",\n",
    "\n",
    "    # APIs and Protocols\n",
    "    \"rest\", \"graphql\", \"grpc\", \"websockets\", \"oauth\", \"jwt\", \"openapi\", \"json\", \"xml\",\n",
    "\n",
    "    # UI/UX Design Tools\n",
    "    \"figma\", \"adobe xd\", \"sketch\", \"invision\", \"zeplin\", \"balsamiq\", \"axure\",\n",
    "\n",
    "    # Mobile Development\n",
    "    \"react native\", \"flutter\", \"ionic\", \"swift\", \"kotlin\", \"objective-c\", \"xamarin\",\n",
    "\n",
    "    # Game Development\n",
    "    \"unity\", \"unreal engine\", \"godot\", \"cocos2d\", \"phaser\", \"panda3d\",\n",
    "\n",
    "    # Content Management Systems\n",
    "    \"wordpress\", \"drupal\", \"joomla\", \"shopify\", \"magento\",\n",
    "\n",
    "    # Other Tools and Platforms\n",
    "    \"eslint\", \"prettier\", \"babel\", \"postman\", \"swagger\", \"jira\", \"asana\", \"trello\", \n",
    "    \"notion\", \"monday.com\", \"visual studio code\", \"intellij idea\", \"pycharm\", \"eclipse\", \n",
    "    \"atom\", \"sublime text\",\n",
    "\n",
    "    # Machine Learning and AI (Optional for Software Devs)\n",
    "    \"tensorflow\", \"keras\", \"pytorch\", \"scikit-learn\", \"pandas\", \"numpy\", \"opencv\", \"nltk\",\n",
    "\n",
    "    # General Development Concepts\n",
    "    \"agile\", \"scrum\", \"kanban\", \"tdd\", \"bdd\", \"design patterns\", \"microservices\",\n",
    "    \"monorepo\", \"modular architecture\", \"performance optimization\", \"web accessibility\",\n",
    "    \"responsive design\", \"seo\", \"cross-browser compatibility\",\n",
    "\n",
    "    # Web Servers\n",
    "    \"nginx\", \"apache\", \"iis\", \"caddy\",\n",
    "\n",
    "    # Security Skills\n",
    "    \"ssl\", \"tls\", \"https\", \"encryption\", \"firewalls\", \"penetration testing\", \"sso\",\n",
    "    \"sast\", \"dast\", \"owasp\",\n",
    "\n",
    "    # Observability and Monitoring\n",
    "    \"prometheus\", \"grafana\", \"splunk\", \"new relic\", \"datadog\", \"elastic stack\", \n",
    "    \"logstash\", \"kibana\",\n",
    "\n",
    "    # Collaboration and Communication Tools\n",
    "    \"slack\", \"microsoft teams\", \"zoom\", \"google meet\", \"discord\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and filter\n",
    "def filter_keywords(keywords):\n",
    "    # Remove duplicates and stop words\n",
    "    keywords = {word.lower() for word in keywords if word.isalpha()}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = keywords - stop_words\n",
    "    # Match with predefined skills\n",
    "    return keywords.intersection(skills_list)\n",
    "\n",
    "filtered_keywords = filter_keywords(cleaned_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rest'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
