{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How's it going? Is there something I can help you with or would you like to chat?\n",
      "Final Response: How's it going? Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def query_ollama_stream(prompt):\n",
    "    \"\"\"Interact with the Ollama server using streaming HTTP responses and overwrite output.\"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"  # Correct endpoint\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1\",  # Replace with your desired model name\n",
    "        \"messages\": [  # Correct key\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Set stream=True to handle streamed responses\n",
    "        with requests.post(url, json=payload, headers=headers, stream=True) as response:\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "\n",
    "            full_response = \"\"  # Initialize a variable to store the complete response\n",
    "            for line in response.iter_lines():\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        # Decode the JSON line\n",
    "                        json_line = json.loads(line.decode(\"utf-8\"))\n",
    "                        # Extract the assistant's content\n",
    "                        content = json_line.get(\"message\", {}).get(\"content\", \"\")\n",
    "                        if content:\n",
    "                            full_response += content  # Append new content\n",
    "                            # Overwrite the line with the updated response\n",
    "                            sys.stdout.write(f\"\\r{full_response}\")\n",
    "                            sys.stdout.flush()\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON line: {e}\")\n",
    "\n",
    "            print()  # Move to a new line after the response is complete\n",
    "            return full_response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Error querying Ollama: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Hi\"\n",
    "    try:\n",
    "        response = query_ollama_stream(prompt)\n",
    "        print(\"Final Response:\", response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SOAP', 'training', 'Information', 'MuleSoft', 'problem', 'APIs', 'broker', 'looking', 'project', 'microservices', 'language', 'Excellent', 'company', 'Degree', 'Communication', 'good', 'Physics', 'similar', 'Microsoft', 'Implementation', 'skills', 'opportunity', 'colleagues', 'office', 'Developer', 'orientation', 'effective', 'message', 'principles', 'following', 'Curiosity', 'documentation', 'Genoa', 'Research', 'study', 'months', 'APEX', 'recent', 'management', 'tools', 'fields', 'cycle', 'innovative', 'proactivity', 'graduates', 'Axpo', 'teams', 'area', 'Computer', 'relevant', 'experience', 'methodologies', 'software', 'Technology', 'Good', 'implemented', 'custom', 'process', 'technical', 'Interest', 'Science', 'Strong', 'Salesforce', 'Mathematics', 'job', 'science', 'new', 'applications', 'Knowledge', 'profile', 'Java', 'architecture', 'Azure', 'business', 'computer', 'resource', 'Italian', 'development', 'languages', 'success', 'Requirements', 'join', 'ensure', 'Developing', 'Visualforce', 'Python', 'activities', 'release', 'C', 'knowledge', 'communication', 'period', 'models', 'organised', 'external', 'Solution', 'team', 'modernly', 'design', 'support', 'consultancy', 'solving', 'optimization', 'department', 'complete', 'functional', 'Italia', 'initial', 'Collaboration', 'learning', 'code', 'Management', 'REST', 'SW', 'suppliers', 'Analysis', 'carry', 'analysts', 'Ecosystem', 'aptitude', 'Engineering', 'solutions', 'welcome', 'SQL', 'Delivery', 'Drafting', 'English', 'multinational', 'appreciated', 'analytical', 'technologies'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process_job_description():\n",
    "    # Step 1: Read the contents of the offer.txt file\n",
    "    with open('offer.txt', 'r') as file:\n",
    "        job_description = file.read()\n",
    "    \n",
    "    doc = nlp(job_description)\n",
    "    keywords = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return set(keywords)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def get_professional_skills(keywords):\n",
    "    # Convert keywords to a single string\n",
    "    keywords_str = ', '.join(keywords)\n",
    "    \n",
    "    # Define the prompt for the Ollama model\n",
    "    prompt = f\"Extract professional skills from the following keywords: {keywords_str}\"\n",
    "    \n",
    "    # Call the Ollama model\n",
    "    response = ollama.ask(prompt)\n",
    "    \n",
    "    # Extract professional skills from the response\n",
    "    professional_skills = response.get('professional_skills', [])\n",
    "    \n",
    "    return professional_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ollama' has no attribute 'ask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m process_job_description()\n\u001b[1;32m----> 4\u001b[0m     professional_skills \u001b[38;5;241m=\u001b[39m \u001b[43mget_professional_skills\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(professional_skills)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mget_professional_skills\u001b[1;34m(keywords)\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract professional skills from the following keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Call the Ollama model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m(prompt)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract professional skills from the response\u001b[39;00m\n\u001b[0;32m     13\u001b[0m professional_skills \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofessional_skills\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ollama' has no attribute 'ask'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = process_job_description()\n",
    "    professional_skills = get_professional_skills(keywords)\n",
    "    print(professional_skills)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
